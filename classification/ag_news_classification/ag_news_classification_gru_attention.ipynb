{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 28.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/conda/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.53.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.1.post20201107)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=5a6677aaf03950e2aa1c53bf84a6dfb028cc3b167805260bdd69a891643bacb3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xce3t4j7/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on gpu!!!\n",
      "running on gpu!!!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"running on gpu!!!\")\n",
    "else:\n",
    "    print(\"cpu :(\")\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "        print(\"running on colab\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        import sys\n",
    "        sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/ag_news\")\n",
    "\n",
    "from utils import ModelJob, Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, \n",
    "                vocab_size, min_frequency, \n",
    "                mode=\"train\", vocab=None):\n",
    "        super(AgNewsDataset).__init__()\n",
    "        logging.info(\"reading dataframe\")\n",
    "        self.df = df\n",
    "        self.nlp = spacy.load(name=\"en_core_web_sm\")\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        logging.info(f\"mode: {mode}\")\n",
    "        if mode == \"train\":\n",
    "            logging.info(\"preprocessing dataframe\")\n",
    "            self.preprocess_df()\n",
    "            logging.info(\"creating vocabulary\")\n",
    "            self.vocab = self.build_vocab()\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "            self.preprocess_df()\n",
    "            logging.info(\"preprocessing dataframe\")\n",
    "        logging.info(\"converting tokens to index\")\n",
    "        self.df[\"text_idx\"] = self.df[\"processed\"].apply(lambda ts: [self.vocab.get(t, self.vocab_size) \n",
    "                                                                                     for t in ts])\n",
    "        self.df[\"len\"] = self.df[\"text_idx\"].apply(len)\n",
    "    def preprocess_df(self):\n",
    "        self.df.columns = map(lambda x: x.lower(), self.df.columns)\n",
    "        self.df[\"text\"] = self.df[\"title\"] + \" \" + self.df[\"description\"]\n",
    "        self.df[\"text\"] = self.df[\"text\"].str.lower()\n",
    "        self.df[\"processed\"] = self.df[\"text\"].apply(self.preprocess)\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "        text = \" \".join(text.split())\n",
    "        doc = self.nlp(text, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\",\"ner\"])\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
    "        return lemmas\n",
    "    \n",
    "    def build_vocab(self):\n",
    "        freq_dict = dict()\n",
    "        for index, row in self.df.iterrows():\n",
    "            for token in row[\"processed\"]:\n",
    "                freq_dict[token] = freq_dict.get(token, 0)+1\n",
    "        freq_dict = [(word, frequency) for word, frequency in freq_dict.items()\n",
    "                    if frequency >= self.min_frequency]\n",
    "        freq_dict = sorted(freq_dict, key = lambda x: x[1], reverse=True)\n",
    "        freq_dict = freq_dict[:self.vocab_size]\n",
    "        freq_dict = dict(freq_dict)\n",
    "        vocab = {token : idx+1 for idx, (token, _) in enumerate(freq_dict.items())}\n",
    "        return vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.df.iloc[idx][\"text_idx\"]\n",
    "        length = self.df.iloc[idx][\"len\"]\n",
    "        y = torch.tensor(self.df.iloc[idx][\"class index\"]-1)\n",
    "        return {\"X\": X,\n",
    "                \"lengths\": length,\n",
    "                \"y\": y}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    X = [torch.tensor(row[\"X\"]) for row in batch]\n",
    "    lengths = [torch.tensor(row[\"lengths\"]) for row in batch]\n",
    "    y = [torch.tensor(row[\"y\"]) for row in batch]\n",
    "    X, y = pad_sequence(X, batch_first=True, padding_value=0), torch.tensor(y)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return X.to(device), lengths, y.to(device)\n",
    "    \n",
    "class NewsClassifierModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size,fc_units, num_classes):\n",
    "        super(NewsClassifierModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1,\n",
    "                                      embedding_dim=100, padding_idx=0)\n",
    "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
    "                          hidden_size=hidden_size,\n",
    "                          bidirectional=True, \n",
    "                          batch_first=True)\n",
    "        self.attention = Attention(input_dim=2*hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(in_features=2*hidden_size, \n",
    "                           out_features=fc_units)\n",
    "        self.out = nn.Linear(in_features=fc_units,\n",
    "                            out_features=4)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        (sequence, lengths) = x\n",
    "        \n",
    "        emb = self.embedding(sequence)\n",
    "        emb_packed = pack_padded_sequence(emb, lengths=lengths, \n",
    "                                          batch_first=True, enforce_sorted=False)\n",
    "        output, h_t = self.rnn(emb_packed)\n",
    "        output_padded, lengths_unpacked = pad_packed_sequence(output, batch_first=True, \n",
    "                                           padding_value=0)\n",
    "        mask = sequence!=0\n",
    "        context, attention_weights = self.attention((output_padded, mask))\n",
    "        hidden_states_dp = self.dropout(context)\n",
    "        fc_out = F.relu(self.fc(hidden_states_dp))\n",
    "        out = self.out(fc_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Read Dataframe\n",
      "INFO:root:Train Test Split\n",
      "INFO:root:Creating Datasets\n",
      "INFO:root:reading dataframe\n",
      "INFO:root:mode: train\n",
      "INFO:root:preprocessing dataframe\n",
      "INFO:root:creating vocabulary\n",
      "INFO:root:converting tokens to index\n",
      "INFO:root:reading dataframe\n",
      "INFO:root:mode: test\n",
      "INFO:root:preprocessing dataframe\n",
      "INFO:root:converting tokens to index\n",
      "INFO:root:Dataset lengths:: train: 37500, test: 12500\n",
      "INFO:root:Started Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.7427667379379272 : ACCURACY: 0.7164500951766968\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.49107635021209717 : ACCURACY: 0.8417055606842041\n",
      "Best epoch : 1\n",
      "Saving model : ag_news_classification_gru_attention.pth at models/\n",
      "EPOCH: 2 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.4287484586238861 : ACCURACY: 0.8487362861633301\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.42918699979782104 : ACCURACY: 0.8669222593307495\n",
      "Best epoch : 2\n",
      "Saving model : ag_news_classification_gru_attention.pth at models/\n",
      "EPOCH: 3 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.3671325445175171 : ACCURACY: 0.871254026889801\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.38881975412368774 : ACCURACY: 0.8822065591812134\n",
      "Best epoch : 3\n",
      "Saving model : ag_news_classification_gru_attention.pth at models/\n",
      "EPOCH: 4 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.33129212260246277 : ACCURACY: 0.8840305805206299\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.37867164611816406 : ACCURACY: 0.885436475276947\n",
      "Best epoch : 4\n",
      "Saving model : ag_news_classification_gru_attention.pth at models/\n",
      "EPOCH: 5 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.30014169216156006 : ACCURACY: 0.8951732516288757\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.3768683075904846 : ACCURACY: 0.8911644816398621\n",
      "Best epoch : 5\n",
      "Saving model : ag_news_classification_gru_attention.pth at models/\n",
      "EPOCH: 6 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.269152969121933 : ACCURACY: 0.905889093875885\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.38495028018951416 : ACCURACY: 0.885481059551239\n",
      "EPOCH: 7 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.24053899943828583 : ACCURACY: 0.9145271182060242\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.40350914001464844 : ACCURACY: 0.8861551880836487\n",
      "EPOCH: 8 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.21001631021499634 : ACCURACY: 0.9245734214782715\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.4137893617153168 : ACCURACY: 0.885865330696106\n",
      "EPOCH: 9 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.17329590022563934 : ACCURACY: 0.9388697147369385\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.4569289982318878 : ACCURACY: 0.8821545839309692\n",
      "EPOCH: 10 out of 10\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 0.1504587084054947 : ACCURACY: 0.9470045566558838\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: test : LOSS: 0.49486181139945984 : ACCURACY: 0.883074164390564\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        data_path = \"/content/drive/MyDrive/Colab Notebooks/ag_news\"\n",
    "        model_save_path = \"/content/drive/MyDrive/Colab Notebooks/ag_news\"\n",
    "        print(\"running on colab\")\n",
    "    else:\n",
    "        data_path = \"data/ag_news/\"\n",
    "        model_save_path = \"models/\"\n",
    "        print(\"running on local\")\n",
    "    df = pd.read_csv(os.path.join(data_path,\"train.csv\"))\n",
    "    df = df.sample(n=50000, random_state=9)\n",
    "    logging.info(\"Read Dataframe\")\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_train, df_test = train_test_split(df, stratify=df[\"Class Index\"], random_state=9)\n",
    "    df_train.shape, df_test.shape\n",
    "    logging.info(\"Train Test Split\")\n",
    "    \n",
    "    logging.info(\"Creating Datasets\")\n",
    "    train_ds = AgNewsDataset(df=df_train,\n",
    "                    vocab_size=1000, \n",
    "                    min_frequency=25, \n",
    "                    mode=\"train\", vocab=None)\n",
    "    test_ds = AgNewsDataset(df=df_test,\n",
    "            vocab_size=1000, \n",
    "            min_frequency=25, \n",
    "            mode=\"test\", vocab=train_ds.vocab)\n",
    "    logging.info(f\"Dataset lengths:: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    test_dl = DataLoader(test_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    model = NewsClassifierModel(vocab_size=1000, \n",
    "                      embedding_dim=100, \n",
    "                      hidden_size=128,\n",
    "                      fc_units=256,\n",
    "                      num_classes=4\n",
    "                     )\n",
    "    model = model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(params=model.parameters(), lr=0.001)\n",
    "    \n",
    "    model_run =  ModelJob(model=model,\n",
    "                    dataloaders = {\"train\": train_dl, \"test\":test_dl},\n",
    "                    model_save_path = model_save_path,\n",
    "                    model_save_name=\"ag_news_classification_gru_attention.pth\",\n",
    "                    criterion=loss_func,\n",
    "                    optimizer=optimizer,\n",
    "                    n_epochs=10,\n",
    "                    phases=[\"train\", \"test\"],\n",
    "                    )\n",
    "    logging.info(\"Started Training\")\n",
    "    model_run.train_step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
