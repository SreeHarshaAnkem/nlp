{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ag_news_classification_gru.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYzuv_Hdq0J",
        "outputId": "23575d1c-a254-47f2-cc0e-f1d59f4135f8"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.optim import Adam\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    print(\"running on gpu!!!\")\n",
        "else:\n",
        "    print(\"cpu :(\")\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "        print(\"running on colab\")\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        import sys\n",
        "        sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/ag_news\")\n",
        "from utils import ModelJob, Attention"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on gpu!!!\n",
            "running on colab\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on gpu!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nw67DCUd04_"
      },
      "source": [
        "class AgNewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, \n",
        "                vocab_size, min_frequency, \n",
        "                mode=\"train\", vocab=None):\n",
        "        super(AgNewsDataset).__init__()\n",
        "        logging.info(\"reading dataframe\")\n",
        "        self.df = df\n",
        "        self.nlp = spacy.load(name=\"en_core_web_sm\")\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_frequency = min_frequency\n",
        "        logging.info(f\"mode: {mode}\")\n",
        "        if mode == \"train\":\n",
        "            logging.info(\"preprocessing dataframe\")\n",
        "            self.preprocess_df()\n",
        "            logging.info(\"creating vocabulary\")\n",
        "            self.vocab = self.build_vocab()\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "            self.preprocess_df()\n",
        "            logging.info(\"preprocessing dataframe\")\n",
        "        logging.info(\"converting tokens to index\")\n",
        "        self.df[\"text_idx\"] = self.df[\"processed\"].apply(lambda ts: [self.vocab.get(t, self.vocab_size) \n",
        "                                                                                     for t in ts])\n",
        "        self.df[\"len\"] = self.df[\"text_idx\"].apply(len)\n",
        "    def preprocess_df(self):\n",
        "        self.df.columns = map(lambda x: x.lower(), self.df.columns)\n",
        "        self.df[\"text\"] = self.df[\"title\"] + \" \" + self.df[\"description\"]\n",
        "        self.df[\"text\"] = self.df[\"text\"].str.lower()\n",
        "        self.df[\"processed\"] = self.df[\"text\"].apply(self.preprocess)\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "        text = \" \".join(text.split())\n",
        "        doc = self.nlp(text, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\",\"ner\"])\n",
        "        lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
        "        return lemmas\n",
        "    \n",
        "    def build_vocab(self):\n",
        "        freq_dict = dict()\n",
        "        for index, row in self.df.iterrows():\n",
        "            for token in row[\"processed\"]:\n",
        "                freq_dict[token] = freq_dict.get(token, 0)+1\n",
        "        freq_dict = [(word, frequency) for word, frequency in freq_dict.items()\n",
        "                    if frequency >= self.min_frequency]\n",
        "        freq_dict = sorted(freq_dict, key = lambda x: x[1], reverse=True)\n",
        "        freq_dict = freq_dict[:self.vocab_size]\n",
        "        freq_dict = dict(freq_dict)\n",
        "        vocab = {token : idx+1 for idx, (token, _) in enumerate(freq_dict.items())}\n",
        "        return vocab\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        X = self.df.iloc[idx][\"text_idx\"]\n",
        "        length = self.df.iloc[idx][\"len\"]\n",
        "        y = torch.tensor(self.df.iloc[idx][\"class index\"]-1)\n",
        "        return {\"X\": X,\n",
        "                \"lengths\": length,\n",
        "                \"y\": y}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X = [torch.tensor(row[\"X\"]) for row in batch]\n",
        "    lengths = [torch.tensor(row[\"lengths\"]) for row in batch]\n",
        "    y = [torch.tensor(row[\"y\"]) for row in batch]\n",
        "    X, y = pad_sequence(X, batch_first=True, padding_value=0), torch.tensor(y)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    return X.to(device), lengths, y.to(device)\n",
        "    \n",
        "class NewsClassifierModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size,fc_units, num_classes):\n",
        "        super(NewsClassifierModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1,\n",
        "                                      embedding_dim=100, padding_idx=0)\n",
        "        self.rnn = nn.GRU(input_size=embedding_dim,\n",
        "                          hidden_size=hidden_size,\n",
        "                          bidirectional=True, \n",
        "                          batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(in_features=2*hidden_size, \n",
        "                           out_features=fc_units)\n",
        "        self.out = nn.Linear(in_features=fc_units,\n",
        "                            out_features=4)\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "    def forward(self, x):\n",
        "        (sequence, lengths) = x\n",
        "        emb = self.embedding(sequence)\n",
        "        emb_packed = pack_padded_sequence(emb, lengths=lengths, \n",
        "                                          batch_first=True, enforce_sorted=False)\n",
        "        output, h_t = self.rnn(emb_packed)\n",
        "        hidden_states = torch.cat((h_t[-2,:, :], h_t[-1,:, :]), dim=1)\n",
        "        hidden_states_dp = self.dropout(hidden_states)\n",
        "        fc_out = F.relu(self.fc(hidden_states_dp))\n",
        "        out = self.out(fc_out)\n",
        "        return out"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVtHb2jbd4-e",
        "outputId": "ee26a454-f855-485a-b7e2-dbdcdd49c4dd"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    if 'COLAB_GPU' in os.environ:\n",
        "        data_path = \"/content/drive/MyDrive/Colab Notebooks/ag_news\"\n",
        "        print(\"running on colab\")\n",
        "    else:\n",
        "        data_path = \"data/ag_news/\"\n",
        "        print(\"running on local\")\n",
        "    df = pd.read_csv(os.path.join(data_path,\"train.csv\"))\n",
        "    df = df.sample(n=50000, random_state=9)\n",
        "    logging.info(\"Read Dataframe\")\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    df_train, df_test = train_test_split(df, stratify=df[\"Class Index\"], random_state=9)\n",
        "    df_train.shape, df_test.shape\n",
        "    logging.info(\"Train Test Split\")\n",
        "    \n",
        "    logging.info(\"Creating Datasets\")\n",
        "    train_ds = AgNewsDataset(df=df_train,\n",
        "                    vocab_size=1000, \n",
        "                    min_frequency=25, \n",
        "                    mode=\"train\", vocab=None)\n",
        "    test_ds = AgNewsDataset(df=df_test,\n",
        "            vocab_size=1000, \n",
        "            min_frequency=25, \n",
        "            mode=\"test\", vocab=train_ds.vocab)\n",
        "    logging.info(f\"Dataset lengths:: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "    test_dl = DataLoader(test_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "    model = NewsClassifierModel(vocab_size=1000, \n",
        "                      embedding_dim=100, \n",
        "                      hidden_size=128,\n",
        "                      fc_units=256,\n",
        "                      num_classes=4\n",
        "                     )\n",
        "    model = model.to(device)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(params=model.parameters(), lr=0.001)\n",
        "    \n",
        "    model_run =  ModelJob(model=model,\n",
        "                    dataloaders = {\"train\": train_dl, \"test\":test_dl},\n",
        "                    model_save_path = \"/content/drive/MyDrive/Colab Notebooks/ag_news\",\n",
        "                    model_save_name=\"ag_news_classification_gru.pth\",\n",
        "                    criterion=loss_func,\n",
        "                    optimizer=optimizer,\n",
        "                    n_epochs=10,\n",
        "                    phases=[\"train\", \"test\"],\n",
        "                    )\n",
        "    logging.info(\"Started Training\")\n",
        "    model_run.train_step()\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on colab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Read Dataframe\n",
            "INFO:root:Train Test Split\n",
            "INFO:root:Creating Datasets\n",
            "INFO:root:reading dataframe\n",
            "INFO:root:mode: train\n",
            "INFO:root:preprocessing dataframe\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:root:creating vocabulary\n",
            "INFO:root:converting tokens to index\n",
            "INFO:root:reading dataframe\n",
            "INFO:root:mode: test\n",
            "INFO:root:preprocessing dataframe\n",
            "INFO:root:converting tokens to index\n",
            "INFO:root:Dataset lengths:: train: 37500, test: 12500\n",
            "INFO:root:Started Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.7924565076828003 : ACCURACY: 0.6789005398750305\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.49191489815711975 : ACCURACY: 0.8398502469062805\n",
            "EPOCH: 2 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.4246262311935425 : ACCURACY: 0.8501498103141785\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.4204830825328827 : ACCURACY: 0.8699142932891846\n",
            "Best epoch : 1\n",
            "Saving model : ag_news_classification_gru.pth at /content/drive/MyDrive/Colab Notebooks/ag_news\n",
            "EPOCH: 3 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.3638489842414856 : ACCURACY: 0.8724694848060608\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.3876986801624298 : ACCURACY: 0.8798126578330994\n",
            "Best epoch : 2\n",
            "Saving model : ag_news_classification_gru.pth at /content/drive/MyDrive/Colab Notebooks/ag_news\n",
            "EPOCH: 4 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.32611605525016785 : ACCURACY: 0.884719729423523\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.3741648197174072 : ACCURACY: 0.8852550387382507\n",
            "Best epoch : 3\n",
            "Saving model : ag_news_classification_gru.pth at /content/drive/MyDrive/Colab Notebooks/ag_news\n",
            "EPOCH: 5 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.29633915424346924 : ACCURACY: 0.894688069820404\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.37661221623420715 : ACCURACY: 0.8844115138053894\n",
            "EPOCH: 6 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.26460397243499756 : ACCURACY: 0.9045904278755188\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.3835967481136322 : ACCURACY: 0.885553777217865\n",
            "EPOCH: 7 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.23082387447357178 : ACCURACY: 0.9172958135604858\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.39917126297950745 : ACCURACY: 0.8853678703308105\n",
            "EPOCH: 8 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.20212557911872864 : ACCURACY: 0.9271312952041626\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.422004371881485 : ACCURACY: 0.8801747560501099\n",
            "EPOCH: 9 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.16969475150108337 : ACCURACY: 0.9379594326019287\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.46059978008270264 : ACCURACY: 0.8808875679969788\n",
            "EPOCH: 10 out of 10\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.14241132140159607 : ACCURACY: 0.948671817779541\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.49394115805625916 : ACCURACY: 0.8792124390602112\n"
          ]
        }
      ]
    }
  ]
}