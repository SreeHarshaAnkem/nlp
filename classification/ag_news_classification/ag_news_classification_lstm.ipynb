{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ag_news_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-XlrOzuk6Z8",
        "outputId": "01ac33c4-5d18-4254-8309-365e264a4cbf"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q0SFhQcjYjY",
        "outputId": "ca52c5e3-908a-4698-9a2b-6a38206a4977"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from torch.optim import Adam\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from utils import ModelJob, Attention\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    print(\"running on gpu!!!\")\n",
        "else:\n",
        "    print(\"cpu :(\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on gpu!!!\n",
            "running on gpu!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be--nJQ9S2kW"
      },
      "source": [
        "class AgNewsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df, \n",
        "                vocab_size, min_frequency, \n",
        "                mode=\"train\", vocab=None):\n",
        "        super(AgNewsDataset).__init__()\n",
        "        logging.info(\"reading dataframe\")\n",
        "        self.df = df\n",
        "        self.nlp = spacy.load(name=\"en_core_web_sm\")\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_frequency = min_frequency\n",
        "        logging.info(f\"mode: {mode}\")\n",
        "        if mode == \"train\":\n",
        "            logging.info(\"preprocessing dataframe\")\n",
        "            self.preprocess_df()\n",
        "            logging.info(\"creating vocabulary\")\n",
        "            self.vocab = self.build_vocab()\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "            self.preprocess_df()\n",
        "            logging.info(\"preprocessing dataframe\")\n",
        "        logging.info(\"converting tokens to index\")\n",
        "        self.df[\"text_idx\"] = self.df[\"processed\"].apply(lambda ts: [self.vocab.get(t, self.vocab_size) \n",
        "                                                                                     for t in ts])\n",
        "        self.df[\"len\"] = self.df[\"text_idx\"].apply(len)\n",
        "    def preprocess_df(self):\n",
        "        self.df.columns = map(lambda x: x.lower(), self.df.columns)\n",
        "        self.df[\"text\"] = self.df[\"title\"] + \" \" + self.df[\"description\"]\n",
        "        self.df[\"text\"] = self.df[\"text\"].str.lower()\n",
        "        self.df[\"processed\"] = self.df[\"text\"].apply(self.preprocess)\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "        text = \" \".join(text.split())\n",
        "        doc = self.nlp(text, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\",\"ner\"])\n",
        "        lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
        "        return lemmas\n",
        "    \n",
        "    def build_vocab(self):\n",
        "        freq_dict = dict()\n",
        "        for index, row in self.df.iterrows():\n",
        "            for token in row[\"processed\"]:\n",
        "                freq_dict[token] = freq_dict.get(token, 0)+1\n",
        "        freq_dict = [(word, frequency) for word, frequency in freq_dict.items()\n",
        "                    if frequency >= self.min_frequency]\n",
        "        freq_dict = sorted(freq_dict, key = lambda x: x[1], reverse=True)\n",
        "        freq_dict = freq_dict[:self.vocab_size]\n",
        "        freq_dict = dict(freq_dict)\n",
        "        vocab = {token : idx+1 for idx, (token, _) in enumerate(freq_dict.items())}\n",
        "        return vocab\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.df.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        X = self.df.iloc[idx][\"text_idx\"]\n",
        "        length = self.df.iloc[idx][\"len\"]\n",
        "        y = torch.tensor(self.df.iloc[idx][\"class index\"]-1)\n",
        "        return {\"X\": X,\n",
        "                \"lengths\": length,\n",
        "                \"y\": y}\n",
        "\n",
        "def collate_fn(batch):\n",
        "    X = [torch.tensor(row[\"X\"]) for row in batch]\n",
        "    lengths = [torch.tensor(row[\"lengths\"]) for row in batch]\n",
        "    y = [torch.tensor(row[\"y\"]) for row in batch]\n",
        "    X, y = pad_sequence(X, batch_first=True, padding_value=0), torch.tensor(y)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    return X.to(device), lengths, y.to(device)\n",
        "    \n",
        "class NewsClassifierModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size,fc_units, num_classes):\n",
        "        super(NewsClassifierModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size+1,\n",
        "                                      embedding_dim=100, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(input_size=embedding_dim,\n",
        "                          hidden_size=hidden_size,\n",
        "                          bidirectional=True, \n",
        "                          batch_first=True)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.fc = nn.Linear(in_features=2*hidden_size, \n",
        "                           out_features=fc_units)\n",
        "        self.out = nn.Linear(in_features=fc_units,\n",
        "                            out_features=4)\n",
        "        self.num_classes = num_classes\n",
        "        \n",
        "    def forward(self, x):\n",
        "        (sequence, lengths) = x\n",
        "        emb = self.embedding(sequence)\n",
        "        emb_packed = pack_padded_sequence(emb, lengths=lengths, \n",
        "                                          batch_first=True, enforce_sorted=False)\n",
        "        output, [h_t, c_t] = self.rnn(emb_packed)\n",
        "        hidden_states = torch.cat((h_t[-2,:, :], h_t[-1,:, :]), dim=1)\n",
        "        hidden_states_dp = self.dropout(hidden_states)\n",
        "        fc_out = F.relu(self.fc(hidden_states_dp))\n",
        "        out = self.out(fc_out)\n",
        "        return out"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y23kfQiPTNk0",
        "outputId": "c25147e0-1190-494f-8bde-dafc11b5c33c"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data_path = \"/content/drive/MyDrive/Colab Notebooks/ag_news\"\n",
        "    df = pd.read_csv(os.path.join(data_path,\"train.csv\"))\n",
        "    df = df.sample(n=50000, random_state=9)\n",
        "    logging.info(\"Read Dataframe\")\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    df_train, df_test = train_test_split(df, stratify=df[\"Class Index\"], random_state=9)\n",
        "    df_train.shape, df_test.shape\n",
        "    logging.info(\"Train Test Split\")\n",
        "    \n",
        "    logging.info(\"Creating Datasets\")\n",
        "    train_ds = AgNewsDataset(df=df_train,\n",
        "                    vocab_size=1000, \n",
        "                    min_frequency=25, \n",
        "                    mode=\"train\", vocab=None)\n",
        "    test_ds = AgNewsDataset(df=df_test,\n",
        "            vocab_size=1000, \n",
        "            min_frequency=25, \n",
        "            mode=\"test\", vocab=train_ds.vocab)\n",
        "    logging.info(f\"Dataset lengths:: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "    test_dl = DataLoader(test_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "    \n",
        "    model = NewsClassifierModel(vocab_size=1000, \n",
        "                      embedding_dim=100, \n",
        "                      hidden_size=128,\n",
        "                      fc_units=256,\n",
        "                      num_classes=4\n",
        "                     )\n",
        "    model = model.to(device)\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(params=model.parameters(), lr=0.001)\n",
        "    \n",
        "    model_run =  ModelJob(model=model,\n",
        "                    dataloaders = {\"train\": train_dl, \"test\":test_dl},\n",
        "                    criterion=loss_func,\n",
        "                    optimizer=optimizer,\n",
        "                    n_epochs=5,\n",
        "                    phases=[\"train\", \"test\"],\n",
        "                    )\n",
        "    logging.info(\"Started Training\")\n",
        "    model_run.train_step()\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Read Dataframe\n",
            "INFO:root:Train Test Split\n",
            "INFO:root:Creating Datasets\n",
            "INFO:root:reading dataframe\n",
            "INFO:root:mode: train\n",
            "INFO:root:preprocessing dataframe\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:root:creating vocabulary\n",
            "INFO:root:converting tokens to index\n",
            "INFO:root:reading dataframe\n",
            "INFO:root:mode: test\n",
            "INFO:root:preprocessing dataframe\n",
            "INFO:root:converting tokens to index\n",
            "INFO:root:Dataset lengths:: train: 37500, test: 12500\n",
            "INFO:root:Started Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 1 out of 5\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.8681720495223999 : ACCURACY: 0.63991779088974\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.5587812066078186 : ACCURACY: 0.8093162775039673\n",
            "EPOCH: 2 out of 5\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.4529595971107483 : ACCURACY: 0.839598536491394\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.44855937361717224 : ACCURACY: 0.8596572875976562\n",
            "EPOCH: 3 out of 5\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.3783169388771057 : ACCURACY: 0.8670237064361572\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.39167848229408264 : ACCURACY: 0.8781538009643555\n",
            "EPOCH: 4 out of 5\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.3304809331893921 : ACCURACY: 0.8847780823707581\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.37641188502311707 : ACCURACY: 0.8869332671165466\n",
            "EPOCH: 5 out of 5\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: train : LOSS: 0.29795756936073303 : ACCURACY: 0.8956215381622314\n",
            "|||||||||||||||||||||||||||||||||||||||||||||||||\n",
            "\tMODE: test : LOSS: 0.38597989082336426 : ACCURACY: 0.8834875226020813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--PMRQA-TfON"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}