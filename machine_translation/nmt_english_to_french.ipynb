{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T13:38:23.951492Z",
     "start_time": "2021-10-22T13:38:23.875727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cpu :(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam\n",
    "import re\n",
    "import spacy\n",
    "import fr_core_news_sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(\"running on gpu!!!\")\n",
    "else:\n",
    "    print(\"cpu :(\")\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "        print(\"running on colab\")\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        import sys\n",
    "        sys.path.append(\"/content/drive/MyDrive/Colab Notebooks/ag_news\")\n",
    "\n",
    "from utils import ModelJob, Attention, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T17:49:04.617661Z",
     "start_time": "2021-10-22T17:49:04.502855Z"
    }
   },
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, df, src_lang, tgt_lang, \n",
    "                 src_vocab_size, tgt_vocab_size, \n",
    "                 min_frequency, \n",
    "                 mode=\"train\", src_vocab=None, tgt_vocab=None):\n",
    "        super(TranslationDataset, self).__init__()\n",
    "        self.df = df\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "        self.src_nlp = spacy.load(name=src_lang+\"_core_web_sm\")\n",
    "        self.tgt_nlp = fr_core_news_sm.load() #spacy.load(name=tgt_lang+\"_core_web_sm\")\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        self.mode = mode\n",
    "        self.df[self.src_lang+\"_tokens\"] = self.df[self.src_lang].apply(lambda x: self.preprocess(x, flag=\"src\"))\n",
    "        self.df[self.src_lang+\"_tokens\"] = self.df[self.src_lang+\"_tokens\"].apply(lambda x: [\"<SOS>\"] + x + [\"<EOS>\"])\n",
    "        self.df[self.tgt_lang+\"_tokens\"] = self.df[self.tgt_lang].apply(lambda x: self.preprocess(x, flag=\"tgt\"))\n",
    "        self.df[self.tgt_lang+\"_tokens\"] = self.df[self.tgt_lang+\"_tokens\"].apply(lambda x: [\"<SOS>\"] + x + [\"<EOS>\"])\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            logging.info(\"Creating vocabulary\")\n",
    "            logging.info(\"Creating source vocabulary\")\n",
    "            self.src_vocab = Vocab(df=self.df, \n",
    "                                   min_frequency=self.min_frequency, \n",
    "                                   vocab_size=self.src_vocab_size, \n",
    "                                   field=self.src_lang+\"_tokens\",\n",
    "                                   include_unk=False)\n",
    "            logging.info(\"Creating target vocabulary\")\n",
    "            self.tgt_vocab = Vocab(df=self.df, \n",
    "                                   min_frequency=self.min_frequency, \n",
    "                                   vocab_size=self.tgt_vocab_size, \n",
    "                                   field=self.tgt_lang+\"_tokens\",\n",
    "                                   include_unk=False)\n",
    "        else:\n",
    "            self.src_vocab = src_vocab\n",
    "            self.tgt_vocab = tgt_vocab\n",
    "        self.df[\"enc_input\"] = self.df[self.src_lang+\"_tokens\"].apply(lambda x: self.create_index(tokens=x, flag=\"src\"))\n",
    "        self.df[\"enc_input_len\"] = self.df[\"enc_input\"].apply(len)\n",
    "        self.df = self.df.loc[self.df[\"enc_input_len\"]>0]\n",
    "        self.df[\"decoder_seq\"] = self.df[self.tgt_lang+\"_tokens\"].apply(lambda x: self.create_index(tokens=x, flag=\"tgt\"))\n",
    "        self.df[\"dec_input\"] = self.df[\"decoder_seq\"].apply(lambda x: x[:-1])\n",
    "        self.df[\"dec_input_len\"] = self.df[\"dec_input\"].apply(len)\n",
    "\n",
    "        if mode !=\"predict\":\n",
    "            self.df[\"dec_output\"] = self.df[\"decoder_seq\"].apply(lambda x: x[1:])\n",
    "            self.df[\"dec_output_len\"] = self.df[\"dec_output\"].apply(len)\n",
    "        \n",
    "    def create_index(self, tokens, flag):\n",
    "        if flag == \"src\":\n",
    "            return [self.src_vocab.vocab[t] for t in tokens if t in self.src_vocab.vocab.keys()]\n",
    "        else:\n",
    "            return [self.tgt_vocab.vocab[t] for t in tokens if t in self.tgt_vocab.vocab.keys()]\n",
    "        \n",
    "    def preprocess(self, text, flag):\n",
    "        text = re.sub(\"[^\\w+|\\s]\", \" \", text)\n",
    "        if flag == \"src\":\n",
    "            doc = self.src_nlp(text, disable=[\"ner\", \"tagger\"])\n",
    "        else:\n",
    "            doc = self.tgt_nlp(text, disable=[\"ner\", \"tagger\"])\n",
    "        lemmas = [token.lemma_ for token in doc]\n",
    "        return lemmas\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        df_batch = self.df.iloc[idx]\n",
    "        if self.mode != \"predict\":\n",
    "            return {\"enc_input\": df_batch[\"enc_input\"], \n",
    "                    \"enc_input_len\": df_batch[\"enc_input_len\"],\n",
    "                    \"dec_input\": df_batch[\"dec_input\"],\n",
    "                    \"dec_input_len\": df_batch[\"dec_input_len\"],\n",
    "                    \"dec_output_len\": df_batch[\"dec_output_len\"],\n",
    "                    \"dec_output\": df_batch[\"dec_output\"]}\n",
    "        else:\n",
    "            return {\"enc_input\": df_batch[\"enc_input\"], \n",
    "                    \"enc_input_len\": df_batch[\"enc_input_len\"],\n",
    "                    \"dec_input\": df_batch[\"dec_input\"],\n",
    "                    \"dec_input_len\": df_batch[\"dec_input_len\"]}\n",
    "        \n",
    "def collate_fn(batch, mode=\"train\"):\n",
    "    enc_input = [torch.tensor(row[\"enc_input\"]) for row in batch]\n",
    "    enc_input_len = [row[\"enc_input_len\"] for row in batch]\n",
    "    dec_input = [torch.tensor(row[\"dec_input\"]) for row in batch]\n",
    "    dec_input_len = [row[\"dec_input_len\"] for row in batch]\n",
    "    enc_input = pad_sequence(enc_input, batch_first=True)\n",
    "    dec_input = pad_sequence(dec_input, batch_first=True)\n",
    "    \n",
    "    \n",
    "    if mode == \"predict\":\n",
    "        return (enc_input.to(device), enc_input_len, dec_input.to(device), dec_input_len)\n",
    "    else:\n",
    "        dec_output_len = [row[\"dec_output_len\"] for row in batch]\n",
    "        dec_output = [torch.tensor(row[\"dec_output\"]) for row in batch]\n",
    "        dec_output = pad_sequence(dec_output, batch_first=True)\n",
    "    \n",
    "    return (enc_input.to(device), enc_input_len, dec_input.to(device), dec_input_len), dec_output.reshape(-1).to(device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T17:49:05.189400Z",
     "start_time": "2021-10-22T17:49:05.096931Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, src_vocab_size, src_embedding_dim, enc_hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_embedding = nn.Embedding(num_embeddings=src_vocab_size, \n",
    "                                      embedding_dim=src_embedding_dim,\n",
    "                                      padding_idx=0 )\n",
    "        self.enc_rnn = nn.GRU(input_size=src_embedding_dim,\n",
    "                         hidden_size=enc_hidden_size,\n",
    "                         batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_input, encoder_input_len = x\n",
    "        enc_emb = self.enc_embedding(encoder_input)\n",
    "        enc_emb_packed = pack_padded_sequence(enc_emb, encoder_input_len, \n",
    "                                              enforce_sorted=False,\n",
    "                                             batch_first=True)\n",
    "        enc_output, h_t = self.enc_rnn(enc_emb_packed)\n",
    "        enc_output_padded, enc_output_lens = pad_packed_sequence(enc_output, batch_first=True)\n",
    "        mask = encoder_input!=0\n",
    "        return enc_output_padded, h_t[0], mask\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.w_query = nn.Linear(in_features=input_dim, out_features=input_dim)\n",
    "        self.w_key = nn.Linear(in_features=input_dim, out_features=input_dim)\n",
    "        self.v = nn.Linear(in_features=input_dim, out_features=1)\n",
    "        self.mask_impute = torch.tensor(-9999.0).to(device)\n",
    "\n",
    "    def forward(self, query, key, mask):\n",
    "        # query : pre-attention-decoder last state [bs, hs]\n",
    "        # key: all encoder_states - [bs, ts, hs]\n",
    "        # [bs*ts, hs]\n",
    "        key_reshaped = key.contiguous().view(key.shape[0]*key.shape[1], -1) \n",
    "        # [bs*ts, hs]\n",
    "        key_transformed = self.w_key(key_reshaped)\n",
    "        # [bs, ts, hs]\n",
    "        key_transform_reshape = key_transformed.reshape(key.shape[0], key.shape[1], -1)\n",
    "        # [bs, ts, hs]\n",
    "        transformed_inp = self.w_query(query).unsqueeze(dim=1) + key_transform_reshape\n",
    "        # [bs, ts]\n",
    "        context_vector = self.v(F.tanh(transformed_inp))\n",
    "        context_vector = torch.where(mask.unsqueeze(2)==0, self.mask_impute, context_vector)\n",
    "        # [bs, ts]\n",
    "        alpha = F.softmax(context_vector, dim=1)\n",
    "        # [bs, ts, hs]\n",
    "        alpha_repeated = alpha.repeat(1, 1, self.input_dim)\n",
    "        effective_context = key * alpha_repeated\n",
    "        \n",
    "        # [bs, hs]\n",
    "        return torch.sum(effective_context, dim=1), alpha\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, dec_embedding_dim, dec_hidden_size, dec_fc_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_embedding = nn.Embedding(num_embeddings=tgt_vocab_size, \n",
    "                                          embedding_dim=dec_embedding_dim, \n",
    "                                          padding_idx=0)\n",
    "        self.dec_rnn = nn.GRU(input_size=dec_embedding_dim,\n",
    "                             hidden_size=dec_hidden_size,\n",
    "                             batch_first=True)\n",
    "        self.dec_dropout = nn.Dropout(p=0.2)\n",
    "        self.dec_fc = nn.Linear(in_features=dec_hidden_size,\n",
    "                               out_features=dec_fc_units)\n",
    "\n",
    "    def forward(self, decoder_input, state=None):\n",
    "        dec_embedding = self.dec_embedding(decoder_input.unsqueeze(dim=1))\n",
    "        if state is not None:\n",
    "            output, state = self.dec_rnn(dec_embedding)\n",
    "        else:\n",
    "            output, state = self.dec_rnn(dec_embedding,state)\n",
    "        dec_fc_in = self.dec_dropout(state[0])\n",
    "        fc_out = F.relu(self.dec_fc(dec_fc_in))\n",
    "        return  fc_out\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,  src_vocab_size, src_embedding_dim, \n",
    "                        tgt_vocab_size, tgt_embedding_dim, \n",
    "                        enc_hidden_size, dec_hidden_size, \n",
    "                        dec_fc_units, fc_out):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.encoder = Encoder(src_vocab_size, src_embedding_dim, enc_hidden_size)\n",
    "        self.decoder = Decoder(tgt_vocab_size, tgt_embedding_dim, dec_hidden_size, dec_fc_units)\n",
    "        self.attention = Attention(input_dim=dec_hidden_size)\n",
    "        self.fc_dropout = nn.Dropout(p=0.2)\n",
    "        self.fc = nn.Linear(in_features=enc_hidden_size+dec_hidden_size, out_features=tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, x, mode=\"train\"):\n",
    "        logits = []\n",
    "        enc_input, enc_input_len, dec_input, dec_input_len = x\n",
    "        all_encoder_states, encoder_state, encoder_mask = self.encoder((enc_input, enc_input_len))\n",
    "        for i in range(dec_input.shape[1]):\n",
    "            decoder_state = self.decoder(dec_input[:, i])\n",
    "            effective_state, attention_weights = self.attention(query=decoder_state, \n",
    "                                                                key=all_encoder_states, \n",
    "                                                                mask=encoder_mask)\n",
    "            fc_in = torch.cat((effective_state, decoder_state), dim=-1)\n",
    "            fc_in = self.fc_dropout(fc_in)\n",
    "            fc_out = self.fc(fc_in)\n",
    "            logits.append(fc_out)\n",
    "        logits = torch.stack(logits, dim=1)\n",
    "        if mode == \"train\":\n",
    "            return logits.contiguous().view(logits.shape[0]*logits.shape[1], -1), decoder_state[0]\n",
    "        elif mode == \"interpret\":\n",
    "            return logits, decoder_state[0], attention_weights\n",
    "        else:\n",
    "            return logits, decoder_state[0]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T18:29:20.908215Z",
     "start_time": "2021-10-22T18:10:13.970008Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on local\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Dataset shape: 189114\n",
      "INFO:root:Dataset shape after filtering: 189114\n",
      "INFO:root:Train Test Split\n",
      "INFO:root:Creating Datasets\n",
      "INFO:root:Creating vocabulary\n",
      "INFO:root:Creating source vocabulary\n",
      "INFO:root:Creating target vocabulary\n",
      "INFO:root:Dataset lengths:: train: 141835, test: 47279\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if 'COLAB_GPU' in os.environ:\n",
    "        data_path = \"/content/drive/MyDrive/Colab Notebooks/nmt\"\n",
    "        model_save_path = \"/content/drive/MyDrive/Colab Notebooks/nmt\"\n",
    "        print(\"running on colab\")\n",
    "    else:\n",
    "        data_path = \"data/\"\n",
    "        model_save_path = \"models/\"\n",
    "        print(\"running on local\")\n",
    "    df = pd.read_table(os.path.join(data_path,\"fra.txt\"),header=None)\n",
    "    #df = df.sample(n=500, random_state=9)\n",
    "    df = df.iloc[:, :2]\n",
    "    df.columns = [\"en\", \"fr\"]\n",
    "    df[\"en\"] = df[\"en\"].str.strip()\n",
    "    df[\"fr\"] = df[\"fr\"].str.strip()\n",
    "\n",
    "    logging.info(f\"Dataset shape: {df.shape[0]}\")\n",
    "    subset = [\"i am \", \"i m \",  \n",
    "              \"he is\", \"he s \", \n",
    "              \"she is\", \"she s \",  \n",
    "              \"you are\", \"you re \", \n",
    "              \"we are\", \"we re \"\n",
    "              \"they are\", \"they re \"]\n",
    "              #,  \"i don't\", \"do you\", \n",
    "            # \"i want\"]\n",
    "            \n",
    "    def _filter(text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r\"([.!?])\", r\" \\1\", text)\n",
    "        text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n",
    "        for sub in subset:\n",
    "            if text.lower().startswith(sub):\n",
    "                if len(text.split(\" \")) < 10:\n",
    "                    return 1\n",
    "        return 0\n",
    "    \n",
    "    #df[\"flag\"] = df[\"en\"].apply(_filter)\n",
    "    #df = df.loc[df[\"flag\"]==1]\n",
    "    logging.info(f\"Dataset shape after filtering: {df.shape[0]}\")\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df_train, df_test = train_test_split(df, random_state=9)\n",
    "    logging.info(\"Train Test Split\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Creating Datasets\")\n",
    "    train_ds = TranslationDataset(df=df_train,\n",
    "                            src_lang=\"en\", \n",
    "                            tgt_lang=\"fr\", \n",
    "                            src_vocab_size=500,\n",
    "                            tgt_vocab_size = 500,\n",
    "                            min_frequency=5, \n",
    "                            mode=\"train\", \n",
    "                            src_vocab=None, \n",
    "                            tgt_vocab=None)\n",
    "    \n",
    "    test_ds = TranslationDataset(df=df_test,\n",
    "                            src_lang=\"en\", \n",
    "                            tgt_lang=\"fr\", \n",
    "                            src_vocab_size=500, \n",
    "                            tgt_vocab_size=500,\n",
    "                            min_frequency=20, \n",
    "                            mode=\"test\", \n",
    "                            src_vocab=train_ds.src_vocab, \n",
    "                            tgt_vocab=train_ds.tgt_vocab)\n",
    "    logging.info(f\"Dataset lengths:: train: {len(train_ds)}, test: {len(test_ds)}\")\n",
    "    \n",
    "    \n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_dl = DataLoader(test_ds, batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T21:23:39.796588Z",
     "start_time": "2021-10-22T18:29:20.916088Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Started Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 2.2616491317749023 : ACCURACY: 0.2454225718975067\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.8279727697372437 : ACCURACY: 0.20923177897930145\n",
      "Best epoch : 1\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 2 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.8015875816345215 : ACCURACY: 0.28499871492385864\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.7092512845993042 : ACCURACY: 0.2178775817155838\n",
      "Best epoch : 2\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 3 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.7115893363952637 : ACCURACY: 0.29294994473457336\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.6568694114685059 : ACCURACY: 0.22135819494724274\n",
      "Best epoch : 3\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 4 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.6600439548492432 : ACCURACY: 0.2980237305164337\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.6262590885162354 : ACCURACY: 0.22303727269172668\n",
      "Best epoch : 4\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 5 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.625773549079895 : ACCURACY: 0.30062365531921387\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.6130547523498535 : ACCURACY: 0.22469688951969147\n",
      "Best epoch : 5\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 6 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5990841388702393 : ACCURACY: 0.3036328852176666\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5962752103805542 : ACCURACY: 0.22641626000404358\n",
      "Best epoch : 6\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 7 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5783714056015015 : ACCURACY: 0.3043632209300995\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.584818720817566 : ACCURACY: 0.22533664107322693\n",
      "Best epoch : 7\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 8 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5624204874038696 : ACCURACY: 0.30422013998031616\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5757789611816406 : ACCURACY: 0.22689077258110046\n",
      "Best epoch : 8\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 9 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5488171577453613 : ACCURACY: 0.30701589584350586\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5694342851638794 : ACCURACY: 0.2261316478252411\n",
      "Best epoch : 9\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 10 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5375882387161255 : ACCURACY: 0.3086819648742676\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5668786764144897 : ACCURACY: 0.23032686114311218\n",
      "Best epoch : 10\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 11 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5267359018325806 : ACCURACY: 0.3090159595012665\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5590816736221313 : ACCURACY: 0.2292056679725647\n",
      "Best epoch : 11\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 12 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5178031921386719 : ACCURACY: 0.30948197841644287\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5601212978363037 : ACCURACY: 0.2289508879184723\n",
      "EPOCH: 13 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.509554386138916 : ACCURACY: 0.309209942817688\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5551049709320068 : ACCURACY: 0.2305830419063568\n",
      "Best epoch : 13\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 14 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.5020209550857544 : ACCURACY: 0.3113754391670227\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5553250312805176 : ACCURACY: 0.22846516966819763\n",
      "EPOCH: 15 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.4964741468429565 : ACCURACY: 0.3117920756340027\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5548168420791626 : ACCURACY: 0.22851155698299408\n",
      "Best epoch : 15\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 16 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.48835289478302 : ACCURACY: 0.31241434812545776\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5500743389129639 : ACCURACY: 0.23096922039985657\n",
      "Best epoch : 16\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 17 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.483959674835205 : ACCURACY: 0.31227225065231323\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.548069953918457 : ACCURACY: 0.23018908500671387\n",
      "Best epoch : 17\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 18 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.4787884950637817 : ACCURACY: 0.3133401870727539\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5476704835891724 : ACCURACY: 0.23210816085338593\n",
      "Best epoch : 18\n",
      "Saving model : fr_translation.pth at models/\n",
      "EPOCH: 19 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.4747804403305054 : ACCURACY: 0.31337445974349976\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.5494948625564575 : ACCURACY: 0.23037166893482208\n",
      "EPOCH: 20 out of 20\n",
      "||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "\tMODE: train : LOSS: 1.470319151878357 : ACCURACY: 0.313909649848938\n",
      "||||||||||\n",
      "\tMODE: test : LOSS: 1.548021912574768 : ACCURACY: 0.2314910739660263\n"
     ]
    }
   ],
   "source": [
    "network = Seq2Seq(src_vocab_size=train_ds.src_vocab_size, \n",
    "          src_embedding_dim=256, \n",
    "          enc_hidden_size=128,\n",
    "          tgt_vocab_size=train_ds.tgt_vocab_size, \n",
    "          tgt_embedding_dim=100, \n",
    "          dec_hidden_size=128,\n",
    "          dec_fc_units=128,\n",
    "          fc_out=256\n",
    "  )\n",
    "network = network.to(device)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = Adam(params=network.parameters(), lr=0.001)\n",
    "\n",
    "model_run =  ModelJob(model=network,\n",
    "                dataloaders = {\"train\": train_dl, \"test\":test_dl},\n",
    "                model_save_path = model_save_path,\n",
    "                model_save_name=\"fr_translation.pth\",\n",
    "                criterion=loss_func,\n",
    "                optimizer=optimizer,\n",
    "                n_epochs=20,\n",
    "                phases=[\"train\", \"test\"],\n",
    "                )\n",
    "logging.info(\"Started Training\")\n",
    "model_run.train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-22T21:25:29.526360Z",
     "start_time": "2021-10-22T21:23:39.940971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________\n",
      "encoder input :: I can't find Tom anywhere.\n",
      "translated output ::  je ne pouvoir pas de Tom\n",
      "original output :: Je ne trouve Tom nulle part.\n",
      "__________\n",
      "encoder input :: I was talking about you.\n",
      "translated output ::  je parler de vous le de toi\n",
      "original output :: Je parlais de toi.\n",
      "__________\n",
      "encoder input :: I'd rather stand than sit.\n",
      "translated output ::  je vouloir m asseoir depuis que vous me tenir\n",
      "original output :: Je préfère rester debout que d'être assis.\n",
      "__________\n",
      "encoder input :: You're very observant.\n",
      "translated output ::  vous être fort\n",
      "original output :: Vous êtes très observateur.\n",
      "__________\n",
      "encoder input :: I'll call back in twenty minutes.\n",
      "translated output ::  je rappeler en minute\n",
      "original output :: Je rappellerai dans vingt minutes.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    df_sample = test_ds.df.sample(n=1)\n",
    "    encoder_input = df_sample[\"en\"].values[0]\n",
    "    decoder_input = \"\"\n",
    "    temperature=1\n",
    "    for _ in range(10):\n",
    "        print(\"_\", end=\"\")\n",
    "        df_text = pd.DataFrame([encoder_input], columns=[\"en\"])\n",
    "        df_text[\"fr\"] = decoder_input\n",
    "        predict_ds = TranslationDataset(df=df_text,\n",
    "                            src_lang=\"en\", \n",
    "                            tgt_lang=\"fr\", \n",
    "                            src_vocab_size=500, \n",
    "                            tgt_vocab_size=500,\n",
    "                            min_frequency=20, \n",
    "                            mode=\"predict\", \n",
    "                            src_vocab=train_ds.src_vocab, \n",
    "                            tgt_vocab=train_ds.tgt_vocab)\n",
    "        predict_dl = DataLoader(dataset=predict_ds, \n",
    "                                batch_size=1,\n",
    "                                shuffle=True, \n",
    "                                collate_fn=lambda x: collate_fn(x, mode=\"predict\"))\n",
    "        for idx, batch in enumerate(predict_dl):\n",
    "            if idx == 0:\n",
    "                logits, state = model_run.predict_step(predict_dl, if_y=False)\n",
    "            else:\n",
    "                logits, state =  model_run.predict_step(predict_dl, h_t_1=state,  \n",
    "                                                        if_y=False)\n",
    "            predicted_char_idx = torch.multinomial(F.softmax(logits[0, -1]/temperature), num_samples=1).item()\n",
    "            predicted_char = train_ds.tgt_vocab.vocab_ctoi.get(predicted_char_idx, \"<UNK>\")\n",
    "            if predicted_char == \"<EOS>\":\n",
    "                break\n",
    "            #print(predicted_char_idx, predicted_char)\n",
    "            if predicted_char != \"<UNK>\":\n",
    "                decoder_input = decoder_input + \" \" + predicted_char\n",
    "    print()\n",
    "    print(\"encoder input ::\", encoder_input)\n",
    "    print(\"translated output ::\", decoder_input)\n",
    "    print(\"original output ::\", df_sample[\"fr\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
